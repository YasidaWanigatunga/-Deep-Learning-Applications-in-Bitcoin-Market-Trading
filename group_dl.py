# -*- coding: utf-8 -*-
"""Group-DL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JAK8qLMF1zaiSAbRM4yhf599tmy_eCBK
"""

# Step 1: Download and install the TA-Lib library
!wget http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz
!tar -xzf ta-lib-0.4.0-src.tar.gz
!cd ta-lib && ./configure --prefix=/usr && make && sudo make install

# Step 2: Install the TA-Lib Python wrapper
!pip install ta-lib

import pandas as pd
from sklearn.preprocessing import MinMaxScaler
import talib as ta
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.metrics import mean_absolute_error, mean_squared_error

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout

from tensorflow.keras.layers import Input, Dense, LayerNormalization, MultiHeadAttention, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.models import load_model

# Load the dataset
file_path = '/content/BTC-USD.csv'
data = pd.read_csv(file_path)

data.head()

# Columns to add lags for
cols_to_lag = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']
for col in cols_to_lag:
  data[col] = pd.to_numeric(data[col])
  for lag in range(1, 8):
        data[f'{col}_lag{lag}'] = data[col].shift(lag)

data.head()

data.tail()

# Save the updated dataset to a new CSV file
data.to_csv('/content/BTC-USD_with_lags.csv', index=False)

# Load the dataset
file_path = '/content/BTC-USD_with_lags.csv'
df = pd.read_csv(file_path)

df.head()

# Split the dataset into training and test sets
split_ratio = 0.8
split_index = int(len(df) * split_ratio)
train_df = df[:split_index]
test_df = df[split_index:]

# Save the split datasets to CSV files
train_df.to_csv('/content/BTC-USD_train.csv', index=True)
test_df.to_csv('/content/BTC-USD_test.csv', index=True)

print("Training and test datasets saved successfully.")

"""# **Data Preprocessing**"""

# Identify missing values
missing_values = df.isnull().sum()
missing_values

# Replace missing values with 0
df.fillna(0, inplace=True)

# Handle missing values by forward filling, then back filling
df.fillna(method='ffill', inplace=True)
df.fillna(method='bfill', inplace=True)

missing_values = df.isnull().sum()
missing_values

# Normalize the data
scaler = MinMaxScaler()
scaled_columns = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume'] + \
                 [f'{col}_lag{lag}' for col in ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume'] for lag in range(1, 8)]

df[scaled_columns] = scaler.fit_transform(df[scaled_columns])

df['SMA'] = ta.SMA(df['Close'], timeperiod=30)
df['EMA'] = ta.EMA(df['Close'], timeperiod=30)
df['RSI'] = ta.RSI(df['Close'], timeperiod=14)
df['MACD'], df['MACD_Signal'], df['MACD_Hist'] = ta.MACD(df['Close'], fastperiod=12, slowperiod=26, signalperiod=9)

# Convert 'Date' column to datetime
df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
df.dropna(subset=['Date'], inplace=True)
df.set_index('Date', inplace=True)

print(df.head())

# Remove any rows with NaN values generated by technical indicators
df.dropna(inplace=True)

print(df.head())

"""# **Feature Engineering**"""

# Summary statistics
print(df.describe())

# Correlation matrix
corr_matrix = df.corr()

print(corr_matrix)

# Plotting Correlation Matrix
plt.figure(figsize=(16, 6))
sns.heatmap(corr_matrix, annot=True, fmt=".2f")
plt.title('Correlation Matrix')
plt.show()

# Histograms
df.hist(bins=30, figsize=(20, 15))
plt.suptitle('Histograms of All Features')
plt.show()

# Box plots
plt.figure(figsize=(20, 10))
df.boxplot()
plt.title('Box Plot of All Features')
plt.show()

# Time series plots
plt.figure(figsize=(16, 6))
plt.plot(df.index, df['Close'], label='Close')
plt.plot(df.index, df['Open'], label='Open', alpha=0.5)
plt.plot(df.index, df['High'], label='High', alpha=0.5)
plt.plot(df.index, df['Low'], label='Low', alpha=0.5)
plt.legend()
plt.title('Time Series of Price Data')
plt.show()

# Scatter plots for some lagged features and the target variable 'Close'
sns.pairplot(df[['Close', 'Close_lag1', 'Close_lag2', 'Close_lag3', 'Close_lag4', 'Close_lag5', 'Close_lag6', 'Close_lag7']])
plt.suptitle('Scatter Plot Matrix for Close and its Lags')
plt.show()

"""# **Model Development**"""

# Prepare the data for LSTM and Transformer
def create_sequences(data, seq_length):
    X, y = [], []
    for i in range(len(data) - seq_length):
        X.append(data[i:i + seq_length])
        y.append(data[i + seq_length])
    return np.array(X), np.array(y)

# Define the sequence length
seq_length = 30

# Select the target feature
target_feature = 'Close'

# Create sequences
X, y = create_sequences(df[target_feature].values, seq_length)

# Split into training and test sets
split = int(0.8 * len(X))
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)

# Reshape data for LSTM
X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))

"""# LSTM model"""

# Define the LSTM model
def create_lstm_model(input_shape, units=50, dropout_rate=0.2):
    model = Sequential()
    model.add(LSTM(units, return_sequences=True, input_shape=input_shape))
    model.add(Dropout(dropout_rate))
    model.add(LSTM(units, return_sequences=False))
    model.add(Dropout(dropout_rate))
    model.add(Dense(1))
    model.compile(optimizer='adam', loss='mean_squared_error')
    return model

# Create the LSTM model
lstm_model = create_lstm_model((seq_length, 1))

# Train the LSTM model
lstm_model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)

# Evaluate the LSTM model
lstm_loss = lstm_model.evaluate(X_test, y_test)
print(f'LSTM Loss: {lstm_loss}')

# Define hyperparameters for random search
units_list = [50, 100]
dropout_rate_list = [0.2, 0.3]
batch_size_list = [32, 64]
epochs_list = [50, 100]

best_loss = float('inf')
best_params = None
best_model = None

for units in units_list:
    for dropout_rate in dropout_rate_list:
        for batch_size in batch_size_list:
            for epochs in epochs_list:
                print(f'Training model with units={units}, dropout_rate={dropout_rate}, batch_size={batch_size}, epochs={epochs}')
                model = create_lstm_model((seq_length, 1), units, dropout_rate)
                model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=0)
                loss = model.evaluate(X_test, y_test, verbose=0)
                print(f'Loss: {loss}')
                if loss < best_loss:
                    best_loss = loss
                    best_params = (units, dropout_rate, batch_size, epochs)
                    best_model = model

print(f'Best parameters: units={best_params[0]}, dropout_rate={best_params[1]}, batch_size={best_params[2]}, epochs={best_params[3]}')
print(f'Best loss: {best_loss}')

from tensorflow.keras.models import load_model

lstm_model_path = 'best_lstm_model.h5'
best_model.save(lstm_model_path)
print(f'LSTM model saved to {lstm_model_path}')

# Load the best LSTM model
lstm_model_path="/content/best_lstm_model.h5"
loaded_lstm_model = load_model(lstm_model_path)
print('LSTM model loaded successfully')

"""# Transformer model"""

# Define the Transformer model
def create_transformer_model(input_shape, head_size=256, num_heads=4, ff_dim=4, num_transformer_blocks=4, dropout=0.2, mlp_units=[128]):
    inputs = Input(shape=input_shape)
    x = inputs
    for _ in range(num_transformer_blocks):
        x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads)(x, x)
        x = Dropout(dropout)(x)
        x = LayerNormalization(epsilon=1e-6)(x)
        x = tf.keras.layers.Dense(ff_dim, activation="relu")(x)
        x = Dropout(dropout)(x)
        x = LayerNormalization(epsilon=1e-6)(x)
    x = tf.keras.layers.GlobalAveragePooling1D()(x)
    for dim in mlp_units:
        x = tf.keras.layers.Dense(dim, activation="relu")(x)
        x = Dropout(dropout)(x)
    outputs = Dense(1)(x)
    model = Model(inputs, outputs)
    model.compile(optimizer='adam', loss='mean_squared_error')
    return model

transformer_model = create_transformer_model((seq_length, 1))

# Train the models
transformer_model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)

"""# **Evaluation**

# Step 1: Evaluate the Model
"""

# Function to evaluate the model
def evaluate_model(model, X_test, y_test):
    predictions = model.predict(X_test)
    mae = mean_absolute_error(y_test, predictions)
    mse = mean_squared_error(y_test, predictions)
    rmse = np.sqrt(mse)
    return mae, mse, rmse, predictions

# Evaluate the loaded LSTM model
lstm_mae, lstm_mse, lstm_rmse, lstm_predictions = evaluate_model(loaded_lstm_model, X_test, y_test)
print(f'LSTM Model Evaluation:\nMAE: {lstm_mae}\nMSE: {lstm_mse}\nRMSE: {lstm_rmse}')

# Define a function to evaluate model performance
def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)

    # Calculate evaluation metrics
    mae = mean_absolute_error(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))

    # Convert predictions and true values to DataFrame
    df_eval = pd.DataFrame({'True': y_test, 'Predicted': y_pred.flatten()})
    df_eval['Date'] = df.index[-len(y_test):]  # assuming y_test is the last part of df

    # Trading strategy: buy if predicted price for next day is higher than current price, otherwise sell
    df_eval['Return'] = np.where(df_eval['Predicted'].shift(-1) > df_eval['True'], df_eval['True'].pct_change(), -df_eval['True'].pct_change())
    df_eval['Cumulative_Return'] = (1 + df_eval['Return']).cumprod() - 1

    # Calculate profitability
    total_return = df_eval['Cumulative_Return'].iloc[-1]
    mean_return = df_eval['Return'].mean()
    std_return = df_eval['Return'].std()

    return mae, rmse, total_return, mean_return, std_return, df_eval

# Evaluate the loaded LSTM model
mae, rmse, total_return, mean_return, std_return, df_eval = evaluate_model(loaded_lstm_model, X_test, y_test)

print(f'LSTM Model Evaluation:')
print(f'Mean Absolute Error (MAE): {mae}')
print(f'Root Mean Squared Error (RMSE): {rmse}')
print(f'Total Return: {total_return}')
print(f'Mean Return: {mean_return}')
print(f'Standard Deviation of Return: {std_return}')

# Plot cumulative returns
plt.figure(figsize=(12, 6))
plt.plot(df_eval['Date'], df_eval['Cumulative_Return'], label='Cumulative Return')
plt.xlabel('Date')
plt.ylabel('Cumulative Return')
plt.title('Cumulative Return of the Trading Strategy')
plt.legend()
plt.show()

"""# **Evaluation**"""

# Function to create sequences for the LSTM model
def create_sequences(data, seq_length):
    X, y = [], []
    for i in range(len(data) - seq_length):
        X.append(data[i:i + seq_length])
        y.append(data[i + seq_length])
    return np.array(X), np.array(y)

"""# Separate test dataset for evaluation"""

test_file_path = '/content/BTC-USD_test.csv'
test_df = pd.read_csv(test_file_path)

# Convert 'Date' column to datetime and set it as index
test_df['Date'] = pd.to_datetime(test_df['Date'], errors='coerce')
test_df.set_index('Date', inplace=True)
test_df.fillna(0, inplace=True)

# Identify missing values
missing_values = test_df.isnull().sum()
missing_values

test_df.head()

# Normalize the data
scaler = MinMaxScaler()
scaled_columns = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume'] + \
                 [f'{col}_lag{lag}' for col in ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume'] for lag in range(1, 8)]

test_df[scaled_columns] = scaler.fit_transform(test_df[scaled_columns])

# Function to create sequences for the LSTM model
def create_sequences(data, seq_length):
    X, y = [], []
    for i in range(len(data) - seq_length):
        X.append(data[i:i + seq_length])
        y.append(data[i + seq_length])
    return np.array(X), np.array(y)

# Define the sequence length
seq_length = 30
target_feature = 'Close'

# Create sequences from the test data
X_test, y_test = create_sequences(test_df[target_feature].values, seq_length)

# Load the best LSTM model
lstm_model_path = "/content/best_lstm_model.h5"
loaded_lstm_model = load_model(lstm_model_path)

# Define a function to evaluate model performance
def evaluate_model(model, X_test, y_test, df):
    y_pred = model.predict(X_test)

    # Calculate evaluation metrics
    mae = mean_absolute_error(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))

    # Convert predictions and true values to DataFrame
    df_eval = pd.DataFrame({'True': y_test, 'Predicted': y_pred.flatten()})
    df_eval['Date'] = df.index[-len(y_test):]  # assuming y_test is the last part of df

    # Trading strategy: buy if predicted price for next day is higher than current price, otherwise sell
    df_eval['Return'] = np.where(df_eval['Predicted'].shift(-1) > df_eval['True'], df_eval['True'].pct_change(), -df_eval['True'].pct_change())

    # Remove any NaN or infinite values in returns
    df_eval['Return'].replace([np.inf, -np.inf], np.nan, inplace=True)
    df_eval.dropna(subset=['Return'], inplace=True)

    # Calculate cumulative return
    df_eval['Cumulative_Return'] = (1 + df_eval['Return']).cumprod() - 1

    # Remove any NaN or infinite values in cumulative return
    df_eval['Cumulative_Return'].replace([np.inf, -np.inf], np.nan, inplace=True)
    df_eval.dropna(subset=['Cumulative_Return'], inplace=True)

    # Calculate profitability
    total_return = df_eval['Cumulative_Return'].iloc[-1] if not df_eval.empty else 0
    mean_return = df_eval['Return'].mean() if not df_eval.empty else 0
    std_return = df_eval['Return'].std() if not df_eval.empty else 0

    return mae, rmse, total_return, mean_return, std_return, df_eval

# Evaluate the loaded LSTM model
mae, rmse, total_return, mean_return, std_return, df_eval = evaluate_model(loaded_lstm_model, X_test, y_test, test_df)

print(f'LSTM Model Evaluation:')
print(f'Mean Absolute Error (MAE): {mae}')
print(f'Root Mean Squared Error (RMSE): {rmse}')
print(f'Total Return: {total_return}')
print(f'Mean Return: {mean_return}')
print(f'Standard Deviation of Return: {std_return}')

# Plot cumulative returns
plt.figure(figsize=(12, 6))
plt.plot(df_eval['Date'], df_eval['Cumulative_Return'], label='Cumulative Return')
plt.xlabel('Date')
plt.ylabel('Cumulative Return')
plt.title('Cumulative Return of the Trading Strategy')
plt.legend()
plt.show()